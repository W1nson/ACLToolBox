,id,title,authorids,authors,TL;DR,abstract,pdf,preprint,consent,paperhash,existing_preprints,software,data,Previous URL,response_PDF,_bibtex
0,8QPwf5LGpe,ALLWAS: Active Learning on Language models in WASserstein space,['aclweb.org/ACL/ARR/2021/July/Paper4/Authors'],['Anonymous'],,"Active learning has emerged as a standard paradigm in areas with scarcity of labeled training data, such as in the medical domain. Language models have emerged as the prevalent choice of several natural language tasks due to the performance boost offered by these models. However, in several domains, such as medicine, the scarcity of labeled training data is a common issue. Also, these models may not work well in cases where class imbalance is prevalent. Active learning may prove helpful in these cases to boost the performance with a limited label budget. To this end, we propose a novel method using sampling techniques based on submodular optimization and optimal transport for active Learning in language models, dubbed ALLWAS. We construct a sampling strategy based on submodular optimization of the designed objective in the gradient domain. Furthermore, to enable learning from few samples, we propose a novel strategy for sampling from the Wasserstein barycenters. Our empirical evaluations on standard benchmark datasets for text classification show that our methods perform significantly better (> 20% relative increase in some cases) than existing approaches for active learning on language models.",/pdf/b513ba5b9a528cdaf5f2be45d335d5fa5b2c0b85.pdf,,,anonymous|allwas_active_learning_on_language_models_in_wasserstein_space,,,,,,
1,jMh_qPcoPH7,The Role of Context in Detecting Previously Fact-Checked Claims,['aclweb.org/ACL/ARR/2021/July/Paper13/Authors'],['Anonymous'],,"Recent years have seen the proliferation of disinformation and misinformation online, thanks to the freedom of expression on the Internet and to the rise of social media. Two solutions were proposed to address the problem: (i) manual fact-checking, which is accurate and credible, but slow and non-scalable, and (ii) automatic fact-checking, which is fast and scalable, but lacks explainability and credibility. With the accumulation of enough manually fact-checked claims, a middle-ground approach has emerged: checking whether a given claim has previously been fact-checked. This can be made automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. This is a relatively new and understudied research direction, and here we focus on claims made in a political debate, where context really matters. Thus, we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and reasoning over the target text using Transformer-XH. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is more important, and can yield 10+ points of absolute improvement.",/pdf/7e9d8c17aff6e7c8b3198dcd4526f5d5390e7181.pdf,,,anonymous|the_role_of_context_in_detecting_previously_factchecked_claims,,/attachment/f660b6c9420a3e9f22ddb42ea9d52b94539dc3e1.zip,/attachment/e5ac214287898e2cec3e27689115bbf7fbbc51aa.zip,,,
2,CCAvbuPYKC,Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem,['aclweb.org/ACL/ARR/2021/July/Paper20/Authors'],['Anonymous'],,"We introduce the task of implicit offensive language detection in dialogues, where a statement may have either an offensive or unoffensive interpretation, depending on the listener and context. We argue that inference is crucial for understanding this broader set of offensive utterances, and create a dataset featuring chains of reasoning to describe how an offensive interpretation may be reached. Experiments show that state-of-the-art methods of offense classification perform poorly on this task, achieving less than 0.12 average accuracy. We explore the use of pre-trained entailment models % to score links as part of a multi-hop approach to the problem, showing improved accuracy in most situations.  We discuss the feasibility of our approach and the types of external knowledge necessary to support it.",/pdf/4401118ef9f149ae9b47226355133954b5929f52.pdf,,,anonymous|rethinking_offensive_text_detection_as_a_multihop_reasoning_problem,,,/attachment/3524145ccdcd6e1777d54b378bfa7570b5d7a6aa.zip,,,
3,h08TpsSm0bI,How to Do Human Evaluation: Best Practices for User Studies in NLP,['aclweb.org/ACL/ARR/2021/July/Paper7/Authors'],['Anonymous'],,"Many research topics in natural language processing (NLP), such as explanation generation, dialog modeling or machine translation, require evaluation that goes beyond standard metrics like accuracy or F1 score toward a more human-centered approach.
Therefore, understanding how to design user studies becomes increasingly important.
However, few comprehensive resources exist on planning, conducting and evaluating user studies for NLP, making it hard to get started for researchers without prior experience in the field of human evaluation.
In this paper, we summarize the most important aspects of user studies and their design and evaluation, providing direct links to NLP tasks and NLP specific challenges where appropriate.
We (i) outline general study design, ethical considerations, and factors to consider for crowdsourcing, (ii) discuss the particularities of user studies in NLP and provide starting points to select questionnaires, experimental designs and evaluation methods that are tailored to the specific NLP tasks.
Additionally, we offer examples with accompanying statistical evaluation code in R throughout, to bridge the gap between theoretical guidelines and practical applications.",/pdf/b5fd08a9166dbe43a1a91ecfde325f37a0ccad06.pdf,,,anonymous|how_to_do_human_evaluation_best_practices_for_user_studies_in_nlp,,/attachment/c656a7cdd1f0c1cde6cdff502ddb64c7ecfc0133.zip,,,,
4,sTp10OxpdBI,A Vector-Based Approach to Few-Shot Veracity Classification for Automated Fact-Checking,['aclweb.org/ACL/ARR/2021/July/Paper15/Authors'],['Anonymous'],,"As progress on automated fact-checking continues to be called, veracity classification has gained more attention. It is the task of predicting the veracity of a given claim by comparing it with retrieved pieces of evidence. One of the challenges for this task is to obtain manual annotations for large datasets, especially when it comes to new domains for which labelled data is unavailable in the first instance. In this paper, we describe a vector-based approach that achieves significant performance improvement on veracity classification in few-shot settings. Performance is compared with two competitive baselines: (1) fine-tuning BERT / RoBERTa, and (2) the state-of-the-art few-shot veracity classification approach leveraging language model perplexity with thresholds. Our approach first utilises sentence-BERT to get sentence vectors of claims and evidences. We then create a relation vector for each claim-evidences pairs, by applying absolute operation on their vector offsets. Experiments show significant improvements over the baselines.   ",/pdf/0b28fb7fde0108fea06ebe49570cbdf8e1ecf733.pdf,,,anonymous|a_vectorbased_approach_to_fewshot_veracity_classification_for_automated_factchecking,,,,,,
5,4IgzCL-ytZs,AlephBERT: Pre-training and End-to-End Language Models Evaluation from Sub-Word to Sentence Level,['aclweb.org/ACL/ARR/2021/July/Paper22/Authors'],['Anonymous'],,"Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported  advances using PLMs in Hebrew are few and far between. The problem is twofold. First, Hebrew resources for training large language models are not at the same order of  magnitude as their English counterparts.  Second, there are no accepted tasks and  benchmarks to evaluate the progress of Hebrew PLMs on, and in particular, evaluation on sub-word (morphological) tasks. We aim to remedy both aspects. We present AlephBERT, a large PLM for Modern Hebrew, trained on larger vocabulary and a larger dataset  than any Hebrew PLM before. Moreover, we introduce a novel language-agnostic architecture that extracts all of the sub-word  morphological segments encoded in contextualized word embedding vectors. Utilizing this new morphological component we offer a new PLM evaluation pipeline of multiple Hebrew tasks and benchmarks, that cover word-level, sub-word level and sentence level tasks. With AlephBERT we achieve state-of-the-art results compared against contemporary baselines. We make our AlephBERT model and evaluation pipeline publicly available, providing a single point of entry for evaluating and comparing Hebrew PLMs.",/pdf/0331b67a6b6cfc3e3072fb14010761d966f2a67c.pdf,,,anonymous|alephbert_pretraining_and_endtoend_language_models_evaluation_from_subword_to_sentence_level,,,,,,
6,iJvRzKAN2d,Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation,['aclweb.org/ACL/ARR/2021/July/Paper23/Authors'],['Anonymous'],,"The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka. code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English,  with ~5M sentence pairs. Subsequently, we propose JAMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of JAMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of JAMT over state-of-the-art code-mixed and robust translation methods.",/pdf/b3237f197651f34be31c39c809696f1a08132833.pdf,,,anonymous|synthetic_data_generation_and_joint_learning_for_robust_codemixed_translation,,/attachment/ca91087596e91ed121cb31d1cf4b9c36e0e4fc17.zip,,,,
7,U54Az93CWPW,Using Language Models on Low-end Hardware,['aclweb.org/ACL/ARR/2021/July/Paper18/Authors'],['Anonymous'],,"This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.",/pdf/6d17ecad03d21967afecdca2290b2e832be35081.pdf,,,anonymous|using_language_models_on_lowend_hardware,,,,,,
8,-koTfmSDsM,A Survey on Geocoding: Algorithms and Datasets for Toponym Resolution,['aclweb.org/ACL/ARR/2021/July/Paper17/Authors'],['Anonymous'],,"Geocoding, the task of converting unstructured text to structured spatial data, has recently seen progress thanks to a variety of new datasets, evaluation metrics, and machine-learning algorithms. We provide a survey to review, organize and analyze recent work on geocoding (also known as toponym resolution) where the text is matched to geospatial coordinates and/or ontologies. We summarize the findings of this research and suggest some promising directions for future work.",/pdf/e9d805385277add9b57b898decacecf87b4573cd.pdf,,,anonymous|a_survey_on_geocoding_algorithms_and_datasets_for_toponym_resolution,,,,https://openreview.net/forum?id=p_ibGwUc8_C,/attachment/de8fc4ad0180e58d5d0ec863448603ffe0eb203f.pdf,
9,Z9arKXstUo5,EIDER: Evidence-enhanced Document-level Relation Extraction,['aclweb.org/ACL/ARR/2021/July/Paper27/Authors'],['Anonymous'],,"Document-level relation extraction (DocRE) aims at extracting the semantic relations among entity pairs in a document. In DocRE, a subset of the sentences in a document, called the evidence sentences, might be sufficient for predicting the relation between a specific entity pair. To make better use of the evidence sentences, in this paper, we propose a three-stage evidence-enhanced DocRE framework called Eider consisting of joint relation and evidence extraction, evidence-centered relation extraction (RE), and fusion of extraction results. We first jointly train an RE model with a simple and memory-efficient evidence extraction model. Then, we construct pseudo documents based on the extracted evidence sentences and run the RE model again. Finally, we fuse the extraction results of the first two stages using a blending layer and make a final prediction. Extensive experiments show that our proposed framework achieves state-of-the-art performance on the DocRED dataset, outperforming the second-best method by 1.37/1.26 Ign F1/F1. In particular, Eider-RoBERTa$_\text{large}$ significantly improves the performance on entity pairs requiring co-reference and multi-hop reasoning by 1.98/2.08 F1, respectively, which cover around 75\% of the cross-sentence samples.",/pdf/b0b193aaa55ad215c3a4afc2aba06622d69bd890.pdf,,,anonymous|eider_evidenceenhanced_documentlevel_relation_extraction,,,,,,
