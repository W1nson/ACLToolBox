,Unnamed: 0,title,authorids,authors,abstract,pdf,software,preprint,consent,consent_to_review,paperhash,TL;DR,existing_preprints,preferred_venue,reviewer/Editor_reassignment_request,reviewer/Editor_reassignment_justification,data,previous_PDF,previous_URL,response_PDF,Previous URL,Previous PDF,Response PDF,_bibtex,venue,venueid
29,BecjRxs-lY5,KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier,['aclweb.org/ACL/ARR/2021/November/Paper708/Authors'],['Anonymous'],"Pre-trained models are widely used in fine-tuning downstream tasks with linear classifiers optimized by the cross entropy loss, which might face robustness and stability problems.
These problems can be improved by learning representations that focus on similarities in the same class and variance in different classes when making predictions.
In this paper, we utilize the K-Nearest Neighbors Classifier in pre-trained model fine-tuning.
For this KNN classifier, we introduce a supervised momentum contrastive learning framework to learn the clustered representations of the supervised downstream tasks.
Extensive experiments on text classification tasks and robustness tests show that by incorporating KNNs with the traditional fine-tuning process, we can obtain significant improvements on the clean accuracy in both rich-source and few-shot settings and can improve the robustness against adversarial attacks.
\footnote{all codes will be available at https://github.com//}",/pdf/969d6630041bf01e6c81ec5f3aa37192db1625de.pdf,,,,,anonymous|knnbert_finetuning_pretrained_models_with_knn_classifier,,,,,,/attachment/4072fddab6e3a0642521092a1d9c29598d73eaac.zip,,,,,,,,,
47,M-A1d9yxSe,A Bit Bayesian Facilitates Efficient Training in Token Classification,['aclweb.org/ACL/ARR/2021/November/Paper1120/Authors'],['Anonymous'],"Token classification is a fundamental subject matter in computational linguistics. Token classification models, like other modern deep neural network models, are usually trained on the entire training set in each epoch, while research has found all of the training data may not be needed in late epochs of training. Inspired by human pedagogy, we propose a teacher-aware structure to accelerate the training of token classification models. After each epoch of training, the teacher samples data that it is uncertain to and data it predicts differently from the student, which are passed into the structure for training in the next epoch. As a proof of concept, we use a Bayesian linear classifier as the teacher, and use two commonly used backbone models as the student. Experiments show that our method reduces the number of training iterations, speeding up the training without affecting the model's performance.",/pdf/e0f11d8561427572a807b765b83de3aad857cd11.pdf,,,,,anonymous|a_bit_bayesian_facilitates_efficient_training_in_token_classification,,,,,,,,,,,,,,,
59,pIV0eeYD6E0,Uncertainty-Based Joint Training For Semi-Supervised Math Word Problem,['aclweb.org/ACL/ARR/2021/November/Paper1822/Authors'],['Anonymous'],"Math word problems (MWPs) convert natural math corpus into structured equation forms. Data sparsity is one of the main obstacles for math word understanding problem due to the high cost of human annotation efforts. However, existing work mainly start from the supervised learning perspective, making the low-resource scenario under explored. In this paper, we are the first to incorporate semi-supervised learning (SSL) framework into MWPs. We propose an uncertainty-aware unlabeled data selection strategies, which can access to reliable samples and increase the model capacity gradually. Besides, to improve the quality of pseudo equations, we incorporate two indirect supervision signals considering the semantic consistency property and grammar format constraints of generated equations. Experimental results on two benchmark MWPs datasets across different ratio of unlabeled data verify the effectiveness and generalization ability of our proposed method.",/pdf/0d4d65a5e5505f4a5a6625167d43478370f0a188.pdf,,,,,anonymous|uncertaintybased_joint_training_for_semisupervised_math_word_problem,,,,,,/attachment/e2d979e747467b27aae762d49ce7296d9dae51d7.zip,,,,,,,,,
77,DsJO6wBxsF-,Probing the Robustness of Trained Metrics for Conversational Dialogue Systems,['aclweb.org/ACL/ARR/2021/November/Paper95/Authors'],['Anonymous'],"This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans. ",/pdf/314c4fff187d0eb1b8b7137563ef562ae4ce8e7a.pdf,/attachment/6173be658a3880fde99ba3d7b47146c3ae44ad1f.zip,,,,anonymous|probing_the_robustness_of_trained_metrics_for_conversational_dialogue_systems,,,,,,/attachment/375a356886f05c0c0229d902b16868b6f54e9232.zip,,,,,,,,,
150,CPdvNQGrOr0,ST-SQL: Semi-Supervised Self-Training for Text-to-SQL via Column Specificity Meta-Learning,['aclweb.org/ACL/ARR/2021/November/Paper2733/Authors'],['Anonymous'],"The few-shot problem is an urgent challenge for the generalization capability of the single-table text-to-SQL task. Current few-shot methods neglect the potential information of unlabeled data and have a domain bias due to the same weight of samples. Motivated by this, this paper proposes a Self-Training text-to-SQL (ST-SQL) method which handles the problem from both views of data and algorithms. At the data level, ST-SQL performs data expansion by using an iterative framework to attach pseudo-labels to unlabeled data. The expanded data are sampled to reversely train the model. At the algorithm level, ST-SQL defines a column specificity to perform a more fine-grained gradient update during meta-training. The common samples are attached more weight to eliminate the domain bias. ST-SQL achieves state-of-the-art results on both open-domain and domain-specific benchmarks and brings more significant improvements on few-shot tests.",/pdf/7690015f02c9c7292e76587d98c792893f21f158.pdf,,,,,anonymous|stsql_semisupervised_selftraining_for_texttosql_via_column_specificity_metalearning,,,,,,,,,,,,,,,
177,oyCPRgwkf5w,Do We Need to Differentiate Negative Candidates Before Training a Neural Ranker?,['aclweb.org/ACL/ARR/2021/November/Paper1621/Authors'],['Anonymous'],"Retrieval-based Question Answering (ReQA) requires a system to find candidates (e.g., sentences or short passages) containing the answer to a given question from a large corpus. A promising way to solve this task is a two-stage pipeline, where the first stage retrieves a set of candidates, and the second stage uses a neural network to rank the retrieved candidates. There are three standard methods to train neural rankers, Binary Cross-Entropy loss, Mean Square Error loss, and Hinge loss. While all these training strategies assign the same label for all the negative candidates, we argue that negativeness is not binary but exists as a spectrum, i.e., some candidates may be more negative than the others, and thus should be treated differently. We present SCONER---scoring negative candidates before training neural ranker---a model trained to differentiate negative candidates. Our approach includes 1) semantic textual similarity-based scoring together with data augmentation for score generation of negative candidates, and 2) a neural ranker trained on data using generated scores as labels. Together, we systematically compare three standard training methods and our proposed method on a range of ReQA datasets under multiple settings (i.e., single-domain and multi-domain). Our finding suggests that using more negative candidates to train neural rankers are better than less in both single- and multi-domain settings, where SCONER is the best in the single-domain settings and Hinge loss is the best in multi-domain settings. ",/pdf/7e798c12ea301b2cdd63dbbfc6b746bce42ffa8c.pdf,,,,,anonymous|do_we_need_to_differentiate_negative_candidates_before_training_a_neural_ranker,,,,,,,,,,,,,,,
180,QTSlG9lWk8,Improving Compositional Generalization with Self-Training for Data-to-Text Generation,['aclweb.org/ACL/ARR/2021/November/Paper1944/Authors'],['Anonymous'],"Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations (MRs). Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata, thereby necessitating few-shot generalization to novel MRs. In this work, we systematically study the compositional generalization of the state-of-the-art T5 models in few-shot data-to-text tasks. We show that T5 models fail to generalize to unseen MRs, and we propose a template-based input representation that considerably improves the model's generalization capability. To further improve the model's performance, we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection. On the commonly-used SGD and Weather benchmarks, the proposed self-training approach improves tree accuracy by $46\%+$ and reduces the slot error rates by $73\%+$ over the strong T5 baselines in few-shot settings.",/pdf/a14c0d46ee2d99ad8428d45d009e605d1ef66610.pdf,,,,,anonymous|improving_compositional_generalization_with_selftraining_for_datatotext_generation,,,,,,,,,,,,,,,
227,Ku8v46wIwL,ShrinkNAS : Single-Path One-Shot Operator Exploratory Training for Transformer with Dynamic Space Shrinking,['aclweb.org/ACL/ARR/2021/November/Paper2484/Authors'],['Anonymous'],"Neural Architecture Search (NAS) for Transformer has shown its growing capabilities in exploiting the benefits of various Transformer architecture configurations. Recent studies envision the diverse potential of introducing unprecedented Transformer operators (OPs, such as Convolution) to its structure, yet the existing methods of doing so are all time-consuming. Traditionally, Single-Path One-Shot (SPOS) models enable efficient search over a vast set of OPs. However, existing SPOS methods on Transformer focus only on dimensional configurations of the vanilla Transformer OP (e.g., Multi-head Attention), and did not consider introducing other OPs. This paper explores the possibility of including OPs in the Transformer-based SPOS architecture search to discover better Transformer structures with the high efficiency facilitated in the SPOS category. To achieve that, we propose Dynamic Space Shrinking  (DSS), a novel method that resolves problems brought from newly added OPs by dynamically keeping the current sample space containing subnets with good configurations and performance. 
We implemented DSS in ShrinkNAS, the first SPOS one-shot inter-OP model for Transformer. Our evaluation shows that ShrinkNAS is of much higher elasticity by finding a better structure beating the human-designed ones under tight constraint (<10M parameters), while existing intra-OP SPOS methods are not even close.",/pdf/f8d6f014290f07af05b7826f6ea9a83d476728c8.pdf,,,,,anonymous|shrinknas_singlepath_oneshot_operator_exploratory_training_for_transformer_with_dynamic_space_shrinking,,,,,,,,,,,,,,,
275,EiosI2K6lU,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,['aclweb.org/ACL/ARR/2021/November/Paper2622/Authors'],['Anonymous'],"In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model.
Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, they did not analyze how the differences of fine-tuning datasets affect on user's detailed impression. 
In addition, the Transformer-based approach has only been verified for English, not for such languages with large inter-language distances as Japanese.
In this study, we develop large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets to examine the effectiveness of the Transformer-based approach for building chit-chat dialogue systems.
We evaluated and analyzed the impressions of human dialogues in different fine-tuning datasets, model parameters, and the use of additional information.",/pdf/4b643270e496aaac0d2e923e9f63519f15ee6d47.pdf,,,,,anonymous|empirical_analysis_of_training_strategies_of_transformerbased_japanese_chitchat_systems,,,,,,/attachment/b5671d4b4b1fb6e32c6b1fa35497c57b3e8343ab.zip,,,,,,,,,
300,4CwYXIpRYe0,CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training,['aclweb.org/ACL/ARR/2021/November/Paper67/Authors'],['Anonymous'],"We propose a novel open-domain question-answering dataset based on the Common Crawl project. With a previously unseen number of around 130 million multilingual question-answer pairs (including about 60 million English data-points), we use our large-scale, natural, diverse and high-quality corpus to in-domain pre-train popular language models for the task of question-answering. In our experiments, we find that our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks.",/pdf/b8254597efc173b9f4e229d81ecf6c21f2764445.pdf,,,,,anonymous|ccqa_a_new_webscale_question_answering_dataset_for_model_pretraining,,,,,,,,,,,,,,,
333,DjU0lWdsAzn,"Two Front-Ends, One Model : Fusing Heterogeneous Speech Features for Low Resource ASR with Multilingual Pre-Training",['aclweb.org/ACL/ARR/2021/November/Paper2504/Authors'],['Anonymous'],"Transfer learning is widely applied in various deep learning-based speech tasks, especially for tasks with a limited amount of data. Recent studies in transfer learning mainly focused on either supervised or self-supervised perspectives. This work, however, seeks to incorporate the two schemes together towards low-resource automatic speech recognition (ASR) for minor and endangered language (EL) communities. We propose a general framework to use learned transformations to resolve time resolution differences between any speech features, allowing for fusion of any self-supervised representations or spectral features used in multilingual pre-training. Our experiments over two low-resource languages and three ELs demonstrate that the proposed framework can significantly improve the absolute average word error rate from 45.4% to 35.5%.",/pdf/802d45fe2d77e28859fccfae0de093d8de52fd17.pdf,,,,,anonymous|two_frontends_one_model_fusing_heterogeneous_speech_features_for_low_resource_asr_with_multilingual_pretraining,,,,,,,,,,,,,,,
368,0n-fDxhaAHj,Cost-Effective Training in Low-Resource Neural Machine Translation,['aclweb.org/ACL/ARR/2021/November/Paper289/Authors'],['Anonymous'],"While Active Learning (AL) techniques are explored in Neural Machine Translation (NMT), only a few works focus on tackling low annotation budgets where a limited number of sentences can get translated. Such situations are especially challenging and can occur for endangered languages with few human annotators or having cost constraints to label large amounts of data. Although AL is shown to be helpful with large budgets, it is not enough to build high-quality translation systems in these low-resource conditions. In this work, we propose a cost-effective training procedure to increase the performance of NMT models utilizing a small number of annotated sentences and dictionary entries. Our method leverages monolingual data with self-supervised objectives and a small-scale, inexpensive dictionary for additional supervision to initialize the NMT model before applying AL. We show that improving the model using a combination of these knowledge sources is essential to exploit AL strategies and increase gains in low-resource conditions. We also present a novel AL strategy inspired by domain adaptation for NMT and show that it is effective for low budgets. We propose a new hybrid data-driven approach, which samples sentences that are diverse from the labelled data as well as similar to unlabelled data. Finally, we show that initializing the NMT model and further using our AL strategy can achieve gains of up to $13$ BLEU compared to conventional AL methods",/pdf/9f9ef5af4cc3b7e6f6cf05082e995f52ee397922.pdf,,,,,anonymous|costeffective_training_in_lowresource_neural_machine_translation,,,,,,,,,,,,,,,
372,qZu4RC-kEZg,$\mathcal{Y}$-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning,['aclweb.org/ACL/ARR/2021/November/Paper2415/Authors'],['Anonymous'],"With the success of large-scale pre-trained models (PTMs), how efficiently adapting PTMs to downstream tasks has attracted tremendous attention, especially for PTMs with billions of parameters. Although some parameter-efficient tuning paradigms have been proposed to address this problem, they still require large resources to compute and store the gradients in the training phase. In this paper, we propose $\mathcal{Y}$-Tuning, an efficient yet effective paradigm to adapt frozen large-scale PTMs to specific downstream tasks. $\mathcal{Y}$-tuning learns dense representations for labels $\mathcal{Y}$ defined in a given task and aligns them to fixed feature representation. Without tuning the features of input text and model parameters, $\mathcal{Y}$-tuning is both parameter-efficient and training-efficient. Although $\mathcal{Y}$-tuning is currently still not comparable with fine-tuning in performance, it has a great advantage in saving computational cost and has the potential to further improve its performance.",/pdf/0b643a4773db14640a687daf18abf67d85a09d97.pdf,/attachment/c077a5abaf4a8cecba9ea12e90353b2e4bc11ac4.zip,,,,anonymous|\mathcalytuning_an_efficient_tuning_paradigm_for_largescale_pretrained_models_via_label_representation_learning,,,,,,,,,,,,,,,
400,otY8XMglGjt,Representation of Ambiguity in Pre-Trained Sentence Embeddings,['aclweb.org/ACL/ARR/2021/November/Paper1492/Authors'],['Anonymous'],"Pre-trained language models have been shown to be very effective for various NLP tasks. All of these models are trained on different datasets and often have different architectures. Simultaneously, various approaches for analysis of what and how is encoded in the layers of these models is also being studied. In this work we focus on ambiguous sentences and examine how they are represented at various layers of BERT and GPT-2 in comparison to unambiguous sentences. Taking ambiguity detection as a probing task, we find that layers of BERT perform better than layers of GPT-2. Using Representational Similarity Analysis, we observe that differences corresponding to varying stimuli emerge in the deeper layers and that for ambiguous sentences the dissimilarity between BERT and GPT-2 representations decreases in deeper layers. We also find that for ambiguous sentences, representational dissimilarity across layers is greater for BERT than for GPT-2.  ",/pdf/6c8eab801c3390d8dc468e8429ddbc2b0e16fd57.pdf,,,,,anonymous|representation_of_ambiguity_in_pretrained_sentence_embeddings,,,,,,,,,,,,,,,
419,TcIIMlhW4eT,AbductionRules: Training Transformers to Explain Unexpected Inputs,['aclweb.org/ACL/ARR/2021/November/Paper660/Authors'],['Anonymous'],"Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability.

This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases.
We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data.
Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work.",/pdf/bb1e3cd0a1571e86388dec045d4b676c7bb275e5.pdf,,,,,anonymous|abductionrules_training_transformers_to_explain_unexpected_inputs,,,,,,,,,,,,,,,
425,Qm_Z1UNDPN_,Pre-Training with Syntactic Structure Prediction for Chinese Semantic Error Recognition,['aclweb.org/ACL/ARR/2021/November/Paper324/Authors'],['Anonymous'],"Existing Chinese text error detection mainly focuses on spelling errors and simple grammatical errors. These errors have been studied extensively and are relatively simple for humans. Chinese Semantic Error Recognition (CSER) pays attention to more complex semantic errors that humans cannot easily recognize compared with Chinese text error detection. Considering the complex syntactic relation between words, we find that syntactic structure from the syntax tree can help identify semantic errors. In this paper, we consider adopting the pre-trained models to solve the task of CSER. To make the model learn syntactic structure in the pre-training stage, we designed a novel pre-training task to predict the syntactic structure from the syntax tree between different words. Due to the lack of a published dataset for CSER, we build a high-quality dataset for CSER for the first time named Corpus of Chinese Linguistic Semantic Acceptability (CoCLSA), which is extracted from the high school examinations. The experimental results on the CoCLSA show that our pre-trained model based on the new pre-training task has a positive performance compared with existing pre-trained models.",/pdf/7f1edeebe9d6e9938de6cf49d87022b7bd028d09.pdf,,,,,anonymous|pretraining_with_syntactic_structure_prediction_for_chinese_semantic_error_recognition,,,,,,,,,,,,,,,
431,gLfSnQcTFgM,Finding the Dominant Winning Ticket in Pre-Trained Language Models,['aclweb.org/ACL/ARR/2021/November/Paper988/Authors'],['Anonymous'],"The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the ""dominant winning ticket""). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.",/pdf/bb70c60f0da7960c30547455de10ba03dc000a37.pdf,,,,,anonymous|finding_the_dominant_winning_ticket_in_pretrained_language_models,,,,,,,,,,,,,,,
436,UFg2s8Yabc7,Training Dynamics for Text Summarization Models,['aclweb.org/ACL/ARR/2021/November/Paper1419/Authors'],['Anonymous'],"Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training models or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on news summarization. Across different datasets (CNN/DM, XSum, MediaSum) and model behaviors (content selection, abstractiveness, hallucination), we study what the model learns at different stages of its fine-tuning process. We find that properties such as copy behavior and content selection are learnt earlier in the training process and these observations are robust across domains. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, and this behavior is more varied across domains. Based on these observations, we demonstrate two techniques for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly. We show that these simple modifications can help achieve different goals, such as improving factuality or improving abstractiveness.
",/pdf/05c02a8aee6b60b470ada4c34f6f1cd3362fcde1.pdf,,,,,anonymous|training_dynamics_for_text_summarization_models,,,,,,,,,,,,,,,
458,c3wXWy6xe3O,Sparsifying Transformer Models with Trainable Representation Pooling,['aclweb.org/ACL/ARR/2021/November/Paper1220/Authors'],['Anonymous'],"We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. 
A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-$k$ operator.
Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being $1.8\times$ faster during training, $4.5\times$ faster during inference and up to $13\times$ more computationally efficient in the decoder.
",/pdf/b8e7a25d39961cf2f23fd4ae3a0c51cc9d98509e.pdf,/attachment/941c17a6fa5f6060a33e78bb09899de41ca83176.zip,,,,anonymous|sparsifying_transformer_models_with_trainable_representation_pooling,,,,,,,/attachment/cadac6f7d8cc7c5b1297018854bde5b4b6afbda3.pdf,https://openreview.net/forum?id=F_tMnw8FAV9,/attachment/a9d352a820fea6783e9fac43e52636dcb67f1026.pdf,,,,,,
473,WtlQ4cal2Y-,Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation,['aclweb.org/ACL/ARR/2021/November/Paper2232/Authors'],['Anonymous'],"Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data. A UNMT model is trained on the pseudo parallel data with $\text{\bf translated source}$, and translates $\text{\bf natural source}$ sentences in inference. The source discrepancy between training and inference hinders the translation performance of UNMT models. By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) $\text{\textit{style gap}}$ (i.e., translated vs. natural text style) that leads to poor generalization capability; (2) $\text{\textit{content gap}}$ that induces the model to produce hallucination content biased towards the target language. To narrow the data gap, we propose an online self-training approach, which simultaneously uses the pseudo parallel data $\{$natural source, translated target$\}$ to mimic the inference scenario. Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines (XLM and MASS) by remedying the style and content gaps.",/pdf/aa6028492e54ae67679e089794db4ca0d0bb631f.pdf,/attachment/6ea518f2249dc78484636b25ca64e1f3d5f41a4f.zip,,,,anonymous|bridging_the_data_gap_between_training_and_inference_for_unsupervised_neural_machine_translation,,,,,,,,,,,,,,,
529,f9gYRaGv934,Analyzing Dynamic Adversarial Training Data in the Limit,['aclweb.org/ACL/ARR/2021/November/Paper1835/Authors'],['Anonymous'],"To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.",/pdf/1a6d0e77105ea3dc3d50ebca32051ee435af8c50.pdf,,,,,anonymous|analyzing_dynamic_adversarial_training_data_in_the_limit,,,,,,,,,,,,,,,
544,46-q5-S-mEF,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System,['aclweb.org/ACL/ARR/2021/November/Paper71/Authors'],['Anonymous'],"Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.",/pdf/8f7097e183cb99d45b28c1b66a0512c8f66cb162.pdf,/attachment/f5256a0846d5e9ca59e3fd8bce1c160a6eeeb9d7.zip,,,,anonymous|multitask_pretraining_for_plugandplay_taskoriented_dialogue_system,,,,,,/attachment/9d1d8aefb29043df4fe638bd0cc8ed3333ca49b5.zip,,,,,,,,,
571,UiSsgooXn_4,Multi-head or Single-head? An Empirical Comparison for Transformer Training,['aclweb.org/ACL/ARR/2021/November/Paper1707/Authors'],['Anonymous'],"Multi-head attention plays a crucial role in the recent success of Transformer, which leads to consistent performance improvements over conventional attention in various applications. The popular belief is that its effectiveness stems from the ability to attend multiple positions jointly. In this paper, we first demonstrate that jointly attending multiple positions is not a unique feature of multi-head attention, as multi-layer single-head attention also attends multiple positions. Then, we suggest the main advantage of multi-head attention is the training stability, since it has fewer layers than the single-head attention when attending the same number of positions. Meanwhile, we show that, with recent advances in deep learning, we can successfully stabilize the training of the deep single-head Transformer. As the training difficulty is no longer a bottleneck, substantially deeper single-head Transformers achieve consistent performance improvements.",/pdf/80c45ade48a36883ab4956140e759686b00eb868.pdf,,,,,anonymous|multihead_or_singlehead_an_empirical_comparison_for_transformer_training,,,,,,,/attachment/94587fba7bd1796066dcf61771c87454c958e2aa.pdf,https://openreview.net/forum?id=kEay9-ECbb-,/attachment/657a0969b0a511329c5d4473522975e337e2d251.pdf,,,,,,
608,H8L5q6lcWqM,Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models,['aclweb.org/ACL/ARR/2021/November/Paper1270/Authors'],['Anonymous'],"Multi-encoder models are a broad family of context-aware neural machine translation systems that aims to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context. We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons. Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently. Secondly, it eases the retrieval of relevant context, since context segments become shorter. We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets. Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.",/pdf/c147fa973d2c12130f623562f09bdadfea5bbfac.pdf,/attachment/ede661ec14ca4e0e5ca1e39d973c48fdca24e55a.zip,,,,anonymous|divide_and_rule_effective_pretraining_for_contextaware_multiencoder_translation_models,,,,,,,/attachment/a4e71d33591035d184f95f69b18dca242c2ced38.pdf,https://openreview.net/forum?id=dYg10Kalyc3,/attachment/71ad020f67c6997d17cf7ac07ef1388ff07a1f5c.pdf,,,,,,
661,MCl8iyuFpFL,Probing and Generalization of Metaphorical Knowledge in Pre-Trained Language Models,['aclweb.org/ACL/ARR/2021/November/Paper2962/Authors'],['Anonymous'],"Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",/pdf/e1f6e7aad96ce1a6416331fa475174a9debc0539.pdf,/attachment/57516f427266aa784a8cbaff6d453b57fd584eee.zip,,,,anonymous|probing_and_generalization_of_metaphorical_knowledge_in_pretrained_language_models,,,,,,/attachment/863a730db775b41f1e94f0576a4fb501582ccdb5.zip,/attachment/77eee6eb8d02b802a0c65ae931f8fe381985158b.pdf,https://openreview.net/forum?id=dhHpdiUiwrP&noteId=QLQioixKh8o&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2021%2FSeptember%2FAuthors%23your-submissions),/attachment/fc921803fddde843b6019fd5d10e2d614cc92842.pdf,,,,,,
676,L4cbpTxl72U,Sequence-to-Sequence Multilingual Pre-Trained Models: A Hope for Low-Resource Language Translation?,['aclweb.org/ACL/ARR/2021/November/Paper2141/Authors'],['Anonymous'],"We investigate the capability of mBART, a sequence-to-sequence multilingual pre-trained model in translating low-resource languages under five factors:  the amount of data used in pre-training the original model, the amount of data used in fine-tuning, the noisiness of the data used for fine-tuning, the domain-relatedness between the pre-training, fine-tuning, and testing datasets, and the language relatedness. When limited parallel corpora are available, fine-tuning mBART can measurably improve translation performance over training Transformers from scratch. mBART effectively uses even domain-mismatched text, suggesting that mBART can learn meaningful representations when data is scarce. Still, it founders when too-small data in unseen languages is provided.",/pdf/be83a5955a151e53248864d2f2e3a730739f89af.pdf,/attachment/51608bd3cbc8604892553ee9294089f7696e1d14.zip,,,,anonymous|sequencetosequence_multilingual_pretrained_models_a_hope_for_lowresource_language_translation,,,,,,,,,,,,,,,
741,bHf273tEkY9,Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,['aclweb.org/ACL/ARR/2021/November/Paper1504/Authors'],['Anonymous'],"Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA.",/pdf/c7ceab2f085d9eb6c655f65993929fa865fec673.pdf,,,,,anonymous|training_data_is_more_valuable_than_you_think_a_simple_and_effective_method_by_retrieving_from_training_data,,,,,,,,,,,,,,,
768,MJfmRtVReXs,Training a Turn-level User Engagingness Predictor for Dialogues with Weak Supervision,['aclweb.org/ACL/ARR/2021/November/Paper197/Authors'],['Anonymous'],"The standard approach to evaluating dialogue engagingness is by measuring conversation turns per session (CTPS), which implies that the dialogue length is the main predictor of the user engagement with a dialogue system. The main limitation of CTPS is that it can be measured only at the session level, i.e., once the dialogue is already over. However, it is crucial for a dialogue system to continuously monitor user engagement throughout the dialogue session as well. Existing approaches to measuring turn-level engagingness require human annotations for training and lack interpretability of their scores. We pioneer an alternative approach, Remaining Depth as Engagingness Predictor (RDEP), which uses the remaining depth (RD) for each turn as the heuristic weak label for engagingness. RDEP does not require human annotations and also relates closely to CTPS, thus serving as a good learning proxy for this metric. In our experiments, we show that RDEP achieves the new state-of-the-art results on the fine-grained evaluation of dialog (FED) dataset (0.38 Spearman) and the Daily-Dialog dataset (0.62 Spearman). ",/pdf/2240000dda3bdf14dcd5807e39fc3a319a23cb02.pdf,,,,,anonymous|training_a_turnlevel_user_engagingness_predictor_for_dialogues_with_weak_supervision,,,,,,,,,,,,,,,
784,cBDrOuONuhl,Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data,['aclweb.org/ACL/ARR/2021/November/Paper658/Authors'],['Anonymous'],"Multi-modal techniques offer significant untapped potential to unlock improved NLP functionality for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world's languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6\% F1-score above models that are trained from scratch.",/pdf/6ffb67c307f94f2ea2dd89c9a25310b93f2393ed.pdf,,,,,anonymous|phoneing_it_in_towards_flexible_multimodal_language_model_training_by_phonetic_representations_of_data,,,,,,,,,,,,,,,
811,36uj9y9UdXS,On the Use of Entity Embeddings from Pre-Trained Language Models for Knowledge Graph Completion,['aclweb.org/ACL/ARR/2021/November/Paper1722/Authors'],['Anonymous'],"Recent work has found that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we explore how to best extract and incorporate those embeddings. We explore the suitability of the extracted embeddings for direct use in entity ranking and introduce both unsupervised and supervised processing methods that can lead to improved downstream performance. We then introduce supervised embedding extraction methods and demonstrate that we can extract more informative representations. We also examine the effect of language model selection and find that the choice of model can have a significant impact. We then synthesize our findings and develop a knowledge graph completion model that significantly outperforms recent neural models.",/pdf/ba1a15c2d2c8c38603342f072511540bc685a7ef.pdf,,,,,anonymous|on_the_use_of_entity_embeddings_from_pretrained_language_models_for_knowledge_graph_completion,,,,,,,,,,,,,,,
956,5KvP-wQF7iT,ATOGAN:Adaptive Training Objective Generative Adversarial Network for Cross-lingual Word Alignment in Non-Isomorphic Embedding Spaces,['aclweb.org/ACL/ARR/2021/November/Paper1720/Authors'],['Anonymous'],"Cross-lingual word alignment is a task for word translation from monolingual word embedding spaces of two languages. Recent works are mostly based on supervised approaches, which need specific bilingual seed dictionaries. The unsupervised adversarial approaches, which utilize the generative adversarial networks to map the whole monolingual space, do not need any aligned data. However these approaches pay no attention to the problem of mode collapse and gradient disappearance in generative adversarial networks(GAN). We proposed an adaptive training objective generative adversarial network(ATOGAN). We combined particle swarm optimization(PSO) with GAN to select the training objective in GAN's training, which alleviates the problem of mode collapse and gradient disappearance. Moreover, we improved the word alignment by bi-directional mapping and consistency loss. Experimental results demonstrate that our approach is better than several state-of-the-art approaches in distant language pairs(non-isomorphic embedding spaces). ",/pdf/4171120d11c6acf7350ca4f815df9de6d977fcd4.pdf,,,,,anonymous|atoganadaptive_training_objective_generative_adversarial_network_for_crosslingual_word_alignment_in_nonisomorphic_embedding_spaces,,,,,,,,,,,,,,,
71,D5u046Zw_2F,Multi-Task End-to-End Training Improves Conversational Recommendation,['aclweb.org/ACL/ARR/2021/October/Paper124/Authors'],['Anonymous'],"In this paper, we analyze the performance of a multitask end-to-end transformer model on the task of conversational recommendations, which aim to provide recommendations based on a user’s explicit preferences expressed in dialogue. While previous works in this area adopt complex multi-component approaches where the dialogue generation and entity recommendation tasks are handled by separate components, we show that a unified transformer model, based on the T5 text-to-text transformer model, can perform competitively in both recommending relevant items and generating conversation dialogue. We fine-tune our model on the ReDIAL conversational movie recommendation dataset, and create additional training tasks derived from MovieLens (such as the prediction of movie attributes and related movies based on an input movie), in a multitask learning setting. Using a series of probe studies, we demonstrate that the learned knowledge in the additional tasks is transferred to the conversational setting, where each task leads to a $9\% - 52\%$ increase in its related probe score.",/pdf/406ab9dbe363aec3caaf2ed2823ff08b0c1fa089.pdf,,,,,anonymous|multitask_endtoend_training_improves_conversational_recommendation,,,,,,,,,,,,,,,
78,HiHA7Ct6a-J,A Two-Stage Curriculum Training Framework for NMT,['aclweb.org/ACL/ARR/2021/October/Paper209/Authors'],['Anonymous'],"Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. Curriculum training aims to present the data to the NMT systems in a meaningful order. In this work, we introduce a two-stage curriculum training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that consider prediction scores of the emerging NMT model. Through extensive experiments on six language pairs comprising low- and high-resource languages from WMT'21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).",/pdf/ecaae7ef4119ae1c3f8b5364a5a0da89c6c8f9de.pdf,,,,,anonymous|a_twostage_curriculum_training_framework_for_nmt,,,,,,,,,,,,,,,
117,KHBzMrQt6-W,"Old BERT, New Tricks: Artificial Language Learning for Pre-Trained Language Models",['aclweb.org/ACL/ARR/2021/October/Paper173/Authors'],['Anonymous'],"We extend the artificial language learning experimental paradigm from psycholinguistics and apply it to pre-trained language models -- specifically, BERT (Devlin et al., 2019). We treat a pretrained model as a subject in an artificial language learning experimental setting: in order to learn the relation between two linguistic properties A and B, we introduce a set of new, non-existent, linguistic items, give the model information about their variation along property A, then measure to what extent the model learns property B for these items as a result of training. We show this method at work for degree modifiers (expressions like *slightly*, *very*, *rather*, *extremely*) and test the hypothesis that the degree expressed by the modifier (low, medium or high degree) is related to its sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). Our experimental results are compatible with existing linguistic observations that relate degree semantics to polarity-sensitivity, including the main one: low degree semantics leads to positive polarity sensitivity (that is, to preference towards affirmative contexts). 
The method can be used in linguistic theory to elaborate on hypotheses and interpret experimental results, as well as for more insightful evaluation of linguistic representations in language models.",/pdf/bb093a012e9a5cb7a37e8cade2fe18415d89ddb5.pdf,,,,,anonymous|old_bert_new_tricks_artificial_language_learning_for_pretrained_language_models,,,,,,,,,,,,,,,
50,dYg10Kalyc3,Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models,['aclweb.org/ACL/ARR/2021/September/Paper10/Authors'],['Anonymous'],"Multi-encoder models are a broad family of context-aware neural machine translation systems that aims to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context. We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons. Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently. Secondly, it eases the retrieval of relevant context, since context segments become shorter. We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets. Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.",/pdf/fead8f44a64b50f4064136aab16f6af00b1bbdde.pdf,/attachment/9700b748f91ddf41910a3a23d029d07f43d53c8c.zip,,,,anonymous|divide_and_rule_effective_pretraining_for_contextaware_multiencoder_translation_models,,,,,,,,,,,,,,,
57,Lc66vCjVidE,Cross-Lingual Event Detection via Optimized Adversarial Training,['aclweb.org/ACL/ARR/2021/September/Paper173/Authors'],['Anonymous'],"In this work, we focus on Cross-Lingual Event Detection (CLED) where a model is trained on data from a source language but its performance is evaluated on data from a second, target, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models (MLM). Their performance, however, reveals there is room for improvement as they mishandle delicate cross-lingual instances. We leverage the use of unlabeled data to train a Language Discriminator (LD) to discern between the source and target languages. The LD is trained in an adversarial manner so that our encoder learns to produce refined, language-invariant representations that lead to improved CLED performance. More importantly, we optimize the adversarial training by only presenting the LD with the most \textit{informative} samples. We base our intuition about \textit{what} makes a sample informative on two disparate metrics: sample similarity and event presence. Thus, we propose using Optimal Transport (OT) as a solution to naturally combine these two distinct information sources into the selection process. Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves new state-of-the-art results.",/pdf/4d23754d1e63c4ca8d6e2047d56a674d19bef38d.pdf,,,,,anonymous|crosslingual_event_detection_via_optimized_adversarial_training,,,,,,,,,,,,,,,
9,DxJHPnz1L0l,Deduplicating Training Data Makes Language Models Better,['aclweb.org/ACL/ARR/2021/August/Paper24/Authors'],['Anonymous'],"We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.
As a result, over $1\%$ of the unprompted output of language models trained on these datasets is copied verbatim from the training data.
We develop two tools that allow us to deduplicate training datasets---for example removing from C4 a single 61 word English sentence that is repeated over $60{,}000$ times.
Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.
We can also reduce train-test overlap, which affects over $4\%$ of the validation set of standard datasets, thus allowing for more accurate evaluation.
",/pdf/f1488964c122f8d56a577dc99cf785a9f38ee6ff.pdf,,,,,anonymous|deduplicating_training_data_makes_language_models_better,,,,,,,,,,,,,,,
0,awL4ru5Fkxq,Predicting Visual Futures with Image Captioning and Pre-Trained Language Models,['aclweb.org/ACL/ARR/2021/Jun/Paper1/Authors'],['Anonymous'],"The task of visual forecasting deals with predicting future events from a sequence of input images. Purely pixel-based approaches find this challenging due to the presence of abstract concepts and temporal events at different timescales. In this paper, we present an approach that combines image captioning with pre-trained language models to predict visual futures. By leveraging language as an intermediate medium, our model is able to perform more effective temporal reasoning on two different tasks -- visual story cloze and action forecasting. Despite making the final predictions using only the generated captions, our approach outperforms state-of-the-art systems by $4\%$ and $6\%$ respectively on the two tasks. We find that our model consistently picks images/actions that are semantically relevant to the given image sequence instead of simply relying on visual similarity.",/pdf/188fed0a10d2d3641f3f64616a5b384f3b2a9147.pdf,/attachment/2c702f7009a3a539c1672d3f1aa139d87a133ad1.zip,,,,anonymous|predicting_visual_futures_with_image_captioning_and_pretrained_language_models,,,,,,,,,,,,,,,
7,wX4qY_TXKp8,Continuation is a Sub-Task of Fill in the Blank: Why Not Train for Both?,['aclweb.org/ACL/ARR/2021/Jun/Paper10/Authors'],['Anonymous'],"The task of inserting text into a specified position in a passage, known as fill in the blank, is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. However, NLG research has mostly focused on continuation models that append text to the end of a passage. Since continuation is in fact a sub-task of fill in the blank, one where the blank is placed at the sequence's end, we propose the training of a single model which can effectively handle both these tasks.
The result is improved efficiency---as only one model needs to be maintained---without any negative impact on performance at either task.",/pdf/b1aea8e2e32bf09533e8d42e9fc3c784046fb912.pdf,/attachment/5a44d4c28a2836eb31762c822b94e20857288069.zip,,,,anonymous|continuation_is_a_subtask_of_fill_in_the_blank_why_not_train_for_both,,,,,,/attachment/e0f543c4a518e244e5e125fb42818f78950fa2f8.zip,,,,,,,,,
10,YSPukpxgWsU,SINA-BERT: A Pre-Trained Language Model for Analysis of Medical Texts in Persian,['aclweb.org/ACL/ARR/2021/May/Paper21/Authors'],['Anonymous'],"We have released SINA-BERT, a language model pre-trained on BERT to address the lack of a high-quality Persian language model in the medical domain. SINA-BERT utilizes pre-training on a large-scale corpus of medical contents including formal and informal texts collected from various online resources in order to improve the performance on health-care related tasks. We employ SINA-BERT to complete following representative tasks: categorization of medical questions, medical sentiment analysis, medical named entity recognition, and medical question retrieval. For each task, we have developed Persian annotated data sets for training and evaluation and learnt a representation for the data of each task especially complex and long medical questions. With the same architecture being used in each task, SINA-BERT outperforms BERT-based models that were previously made available in the Persian language.",/pdf/b600ae3f925eb3f7937b9a73af33137429d8bb8e.pdf,,,,,anonymous|sinabert_a_pretrained_language_model_for_analysis_of_medical_texts_in_persian,,,,,,,,,,,,,,,
