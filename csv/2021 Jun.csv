,id,title,authorids,authors,TL;DR,abstract,pdf,software,preprint,consent,paperhash,existing_preprints,data
0,awL4ru5Fkxq,Predicting Visual Futures with Image Captioning and Pre-Trained Language Models,['aclweb.org/ACL/ARR/2021/Jun/Paper1/Authors'],['Anonymous'],,"The task of visual forecasting deals with predicting future events from a sequence of input images. Purely pixel-based approaches find this challenging due to the presence of abstract concepts and temporal events at different timescales. In this paper, we present an approach that combines image captioning with pre-trained language models to predict visual futures. By leveraging language as an intermediate medium, our model is able to perform more effective temporal reasoning on two different tasks -- visual story cloze and action forecasting. Despite making the final predictions using only the generated captions, our approach outperforms state-of-the-art systems by $4\%$ and $6\%$ respectively on the two tasks. We find that our model consistently picks images/actions that are semantically relevant to the given image sequence instead of simply relying on visual similarity.",/pdf/188fed0a10d2d3641f3f64616a5b384f3b2a9147.pdf,/attachment/2c702f7009a3a539c1672d3f1aa139d87a133ad1.zip,,,anonymous|predicting_visual_futures_with_image_captioning_and_pretrained_language_models,,
1,byOHhLq9Qn,The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications,['aclweb.org/ACL/ARR/2021/Jun/Paper17/Authors'],['Anonymous'],,"Task-oriented dialogue systems in healthcare are attracting increased attention, and have been characterized by a diverse range of architectures and objectives. However, although these systems have been surveyed in the medical community from a non-technical perspective, a systematic review from a rigorous computational perspective remains noticeably absent. As a result, many important implementation details of healthcare-oriented dialogue systems remain limited or under-specified, slowing the pace of innovation in this area.  To fill this gap, we investigated an initial pool of 4070 papers from well-known computer science, natural language processing, and artificial intelligence venues, identifying 70 papers that satisfied our defined inclusion criteria. We conducted a comprehensive technical review of the included papers, and present our findings along with identified trends and intriguing directions for future research.",/pdf/6a5ac14d678715434f62a7869943ad37ae6880af.pdf,,,,anonymous|the_ai_doctor_is_in_a_survey_of_taskoriented_dialogue_systems_for_healthcare_applications,,
2,3sNJ3-U2Bhx,What is Missing in Existing Multi-hop Datasets? Toward Deeper Multi-hop Reasoning Task,['aclweb.org/ACL/ARR/2021/Jun/Paper13/Authors'],['Anonymous'],,"Multi-hop machine reading comprehension (MRC) is a task that requires models to read and perform multi-hop reasoning over multiple paragraphs to answer a question. The task can be used to evaluate reasoning skills, as well as to check the explainability of the models, and is useful in applications (e.g., QA system). However, the current definition of hop (alias step) in the multi-hop MRC is ambiguous; moreover, previous studies demonstrated that many multi-hop examples contain reasoning shortcuts where the questions can be solved without performing multi-hop reasoning. In this opinion paper, we redefine multi-hop MRC to solve the ambiguity of its current definition by providing three different definitions of the steps. Inspired by the assessment of student learning in education, we introduce a new term of In-depth multi-hop reasoning task with three additional evaluations: step evaluation, coreference evaluation, and entity linking evaluation. In addition, we also examine the existing multi-hop datasets based on our proposed definitions. We observe that there is potential to extend the existing multi-hop datasets by including more intermediate evaluations to the task.  To prevent reasoning shortcuts, multi-hop MRC datasets should focus more on providing a clear definition for the steps in the reasoning process and preparing gold data to evaluate them.",/pdf/68ff8b78b4a25901f68ca18ffe26059d33f3f5aa.pdf,,,,anonymous|what_is_missing_in_existing_multihop_datasets_toward_deeper_multihop_reasoning_task,,
3,x_9Efxb1Y8I,Topic-independent Detection of Dissonance in Short Stance Text,['aclweb.org/ACL/ARR/2021/Jun/Paper14/Authors'],['Anonymous'],,"We address dissonance detection, the task of detecting conflicting stance between two input statements.
Computational models for stance detection have typically been trained for a given target topic (e.g. gun control).
In this paper, we aim for building a computational model for dissonance detection without using training data from the topic of test data.
We first build a large-scale dataset of topic-controlled arguments from two sources: (i) an online debate platform, consisting of 15k pairs of statements with support, attack, or no relation from 20 diverse topics, and (ii) Twitter, consisting of 5k pairs of statements from 5 topics.
We then evaluate a BERT-based dissonance detection model on this dataset in a topic-controlled manner.
Our experiments suggest that dissonance detection models learn the topic-independent patterns of language for detecting dissonance and generalize largely to other arguments in unseen topics. ",/pdf/658145419714a35e4c3afc72b7b86a907fba891f.pdf,,,,anonymous|topicindependent_detection_of_dissonance_in_short_stance_text,,
4,UXPTve6W_sF,,,,,,,,,,,,
5,UboX_l0hKrf,On Evaluation and Improvement of Tail Label Performance for Multi-label Text Classification,['aclweb.org/ACL/ARR/2021/Jun/Paper16/Authors'],['Anonymous'],,"Extreme multi-label text classification (XMTC) is a task for tagging each document with the most relevant subset of labels from an extremely large label set. The most challenging part for machine learning methods is the skewed label distribution in which a majority of labels receive very few training instances (named as the tail labels). Benchmark evaluations so far have focused on micro-averaging metrics, where the performance on tail labels can be easily overshadowed by high-frequency labels (named as head labels), and hence they are insufficient for evaluating the true success of methods in XMTC.  This paper presents a re-evaluation of state-of-the-art (SOTA) methods based on the binned macro-averaging F1  instead, which reveals new insights into the strengths and weaknesses of representative methods.  Based on the evaluation, we conduct in-depth analysis and experiments on Transformer models with various depths and attention mechanisms to improve the tail label performance. We show that a shallow Transformer model with word-label attentions can effectively leverage word-level features and outperforms previous Transformers on tails labels.",/pdf/d207eb9212e44a79124bb73a6518c8b51ef9dba8.pdf,/attachment/6f927090a5813b1202b81e8afe7876bd8e3ba744.zip,,,anonymous|on_evaluation_and_improvement_of_tail_label_performance_for_multilabel_text_classification,,
6,Z7JH7C-oxlC,Augmented Neural Story Generation with Commonsense Inference,['aclweb.org/ACL/ARR/2021/Jun/Paper9/Authors'],['Anonymous'],,"Transformer-based language model approaches to automated story generation currently provide state-of-the-art results. However, they still suffer from plot incoherence when generating narratives over time, and critically lack basic commonsense reasoning. Furthermore, existing methods generally focus only on single-character stories, or fail to track characters at all. To improve the coherence of generated narratives and to expand the scope of character-centric narrative generation, we introduce Commonsense-inference Augmented neural StoryTelling (CAST), a framework for introducing commonsense reasoning into the generation process while modeling the interaction between multiple characters. We find that our CAST method produces significantly more coherent and on-topic two-character stories, outperforming baselines in dimensions including plot plausibility and staying on topic. We also show how the CAST method can be used to further train language models that generate more coherent stories and reduce computation cost.",/pdf/8c8ebc2ee3ab80a4e1ebbca5eb94bf8f9b5186a7.pdf,,,,anonymous|augmented_neural_story_generation_with_commonsense_inference,,
7,wX4qY_TXKp8,Continuation is a Sub-Task of Fill in the Blank: Why Not Train for Both?,['aclweb.org/ACL/ARR/2021/Jun/Paper10/Authors'],['Anonymous'],,"The task of inserting text into a specified position in a passage, known as fill in the blank, is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. However, NLG research has mostly focused on continuation models that append text to the end of a passage. Since continuation is in fact a sub-task of fill in the blank, one where the blank is placed at the sequence's end, we propose the training of a single model which can effectively handle both these tasks.
The result is improved efficiency---as only one model needs to be maintained---without any negative impact on performance at either task.",/pdf/b1aea8e2e32bf09533e8d42e9fc3c784046fb912.pdf,/attachment/5a44d4c28a2836eb31762c822b94e20857288069.zip,,,anonymous|continuation_is_a_subtask_of_fill_in_the_blank_why_not_train_for_both,,/attachment/e0f543c4a518e244e5e125fb42818f78950fa2f8.zip
8,lZF0Gyu1thp,Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons,['aclweb.org/ACL/ARR/2021/Jun/Paper12/Authors'],['Anonymous'],,"Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given $k$ systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all ${k \choose 2}$ pairs of systems. However, this can be very expensive as the number of human annotations required would grow quadratically with $k$. In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms. We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%. To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations. Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of human annotations required further by 89%. In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with $k$. Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently. ",/pdf/782d01ef243eb975c87b92775009847e3503837d.pdf,,,,anonymous|active_evaluation_efficient_nlg_evaluation_with_few_pairwise_comparisons,,
9,eiAkrltBTh4,Contrastive Learning of Natural Language and Code Representations for Semantic Code Search,['aclweb.org/ACL/ARR/2021/Jun/Paper23/Authors'],['Anonymous'],,"Retrieving semantically relevant code functions given a natural language (NL) or programming language (PL) query is a task of great practical value towards building productivity enhancing tools for software developers. Recent approaches to solve this task involve leveraging transformer based masked language models that are pre-trained on NL and PL and fine-tuned for code search using a contrastive learning objective. However, these approaches suffer from uninformative in-batch negative samples.

We propose DyHardCode: a contrastive learning framework that leverages hard negative examples, which are mined globally from the entire training corpus to improve the quality of code and natural language representations. We experiment with different hard negative mining strategies, and provide explanations to the effectiveness of our method from the perspectives of optimization and adversarial learning. We show that DyHardCode leads to improvements in multiple code search tasks. Our approach achieves an average (across 6 programming languages) mean reciprocal ranking (MRR) score of $0.750$ as opposed to the previous state of the art result of $0.713$ MRR on the CodeSearchNet benchmark.",/pdf/1c008a66b1b533b3b685b6744a59c15dc8141fba.pdf,,,,anonymous|contrastive_learning_of_natural_language_and_code_representations_for_semantic_code_search,,
10,qzlQNmFSClj,So Different Yet So Alike! Constrained Unsupervised Text Style Transfer,['aclweb.org/ACL/ARR/2021/Jun/Paper8/Authors'],['Anonymous'],,"Transferring text from one domain to the other has seen tremendous progress in the recent past. However, these methods do not aim to explicitly maintain constraints such as similar text length, descriptiveness between the source and the translated text. To this end, we introduce two complementary cooperative losses to the generative adversarial network family. Here, both the generator and the critic reduce the contrastive and/or the classification loss aiming to satisfy the constraints. These losses allow lexical, syntactic, and domain-specific consistencies to persist across domains.  We demonstrate the effectiveness of our method over multiple benchmark datasets, both with single and multi-attribute transfers.  The complimentary cooperative losses also improve text quality across datasets as judged by current, automated generation and human evaluation metrics.",/pdf/192a02b749e04bf8702900b50c602f2f65451798.pdf,,,,anonymous|so_different_yet_so_alike_constrained_unsupervised_text_style_transfer,,
11,RtLKrXkNFmN,Best Practices for Noise-Based Augmentation to Improve the Performance of Deployable Speech-Based Emotion Recognition Systems,['aclweb.org/ACL/ARR/2021/Jun/Paper26/Authors'],['Anonymous'],,"Emotion recognition models are a key component of several downstream applications, such as mental health assessments. These models are usually trained on small, clean, and synthetically controlled datasets, which leads to high failure rates in presence of `unseen' background noises, promoting noise-overlay based adversarial attacks. Noisy data augmentation has aided robustness of speech recognition and classification models, wherein, the ground truth label remains consistent even in the presence of noise which, isn't always true for subjectively perceived emotion labels. In this work, we create realistic noisy samples of IEMOCAP, using multiple categories of environmental and synthetic noise. We evaluate how ground truth labels (human) and predicted labels (model) change as a function of these noise source introductions. We show that some commonly used noisy augmentation techniques, impact human perception of emotion, thus, falsifying the `clean ground truth label. Our experiments show that the performance of both, baseline, and even denoised emotion recognition models significantly declines on noisy samples as compared to that on the clean set. This performance degradation prevails when model is trained on a combination of clean and test set mismatched noisy samples. We investigate how using the above found `human-perceptible noise overlays can lead to inaccurate metrics when testing the model for robustness or vulnerability to adversarial attacks. Finally, we present a set of recommendations for noise-based augmentation of speech emotion datasets and for deploying the models trained using those datasets.",/pdf/451f31f3aab5d5c73b02ba4eb3fe0fb164094a9a.pdf,,,,anonymous|best_practices_for_noisebased_augmentation_to_improve_the_performance_of_deployable_speechbased_emotion_recognition_systems,,
12,tTwG1UKHRB1,A Recipe For Arbitrary Text Style Transfer with Large Language Models,['aclweb.org/ACL/ARR/2021/Jun/Paper6/Authors'],['Anonymous'],,"In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as 'make this melodramatic' or 'insert a metaphor.'",/pdf/ef12e3124cf2bb897c8408e92a856c63068bf33a.pdf,/attachment/15aac78fc10e87da220506ea04487d84cbc48941.zip,,,anonymous|a_recipe_for_arbitrary_text_style_transfer_with_large_language_models,,/attachment/86bb075a89376631cfcd0bba153d1340ea58d5e7.zip
13,gXOlHDiNcs2,Restoring Hebrew Diacritics Without a Dictionary,['aclweb.org/ACL/ARR/2021/Jun/Paper3/Authors'],['Anonymous'],,"We demonstrate that it is feasible to diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present NAKDIMON, a two-layer character level LSTM, that performs on par with much more complicated curation-dependent 
 systems, across a diverse array of modern Hebrew sources.",/pdf/dfc3cdbfdb627d3b0d14da6c0c707bb315af2a43.pdf,,,,anonymous|restoring_hebrew_diacritics_without_a_dictionary,,/attachment/f65aba46f5847ff26ac8d8dde973d992a9f6845c.zip
